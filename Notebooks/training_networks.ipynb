{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides details on how we generated training sets and trained neural networks with paltas. \n",
    "For an introduction to the paltas code base, see the Notebooks folder in the paltas main repo.\n",
    "\n",
    "More scripts I used can be found in the training_scripts folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for generating the NPE training set: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training of CNN for STRIDES30 test\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm, truncnorm\n",
    "import paltas.Sampling.distributions as dist\n",
    "from paltas.MainDeflector.simple_deflectors import PEMDShear\n",
    "from paltas.Sources.sersic import SingleSersicSource\n",
    "from paltas.PointSource.single_point_source import SinglePointSource\n",
    "\n",
    "# calculated using .fits header\n",
    "output_ab_zeropoint = 25.1152\n",
    "\n",
    "kwargs_numerics = {'supersampling_factor':1}\n",
    "\n",
    "# size of cutout\n",
    "numpix = 80\n",
    "\n",
    "# quads_only\n",
    "#doubles_quads_only = True\n",
    "# point source magnification cut\n",
    "#ps_magnification_cut = 2\n",
    "\n",
    "# load in a PSF kernel\n",
    "from astropy.io import fits\n",
    "from lenstronomy.Util import kernel_util\n",
    "\n",
    "#psf_fits_file = '/home/users/sydney3/paltas/datasets/hst_psf/STDPBF_WFC3UV_F814W.fits'\n",
    "psf_fits_file = '/Users/smericks/Desktop/StrongLensing/paltas/datasets/hst_psf/STDPBF_WFC3UV_F814W.fits'\n",
    "\n",
    "# load in focus diverse PSF maps\n",
    "with fits.open(psf_fits_file) as hdu:\n",
    "    psf_kernels = hdu[0].data\n",
    "psf_kernels = psf_kernels.reshape(-1,101,101)\n",
    "psf_kernels[psf_kernels<0] = 0\n",
    "\n",
    "# normalize psf_kernels to sum to 1\n",
    "psf_sums = np.sum(psf_kernels,axis=(1,2))\n",
    "psf_sums = psf_sums.reshape(-1,1,1)\n",
    "normalized_psfs = psf_kernels/psf_sums\n",
    "\n",
    "# pick random weights to create PSF\n",
    "def draw_psf_kernel():\n",
    "\tweights = np.random.uniform(size=np.shape(normalized_psfs)[0])\n",
    "\tweights /= np.sum(weights)\n",
    "\tweighted_sum = np.sum(weights.reshape(len(weights),1,1) * normalized_psfs,axis=0)\n",
    "\treturn kernel_util.degrade_kernel(weighted_sum,4)\n",
    "\n",
    "config_dict = {\n",
    "\t'main_deflector':{\n",
    "\t\t'class': PEMDShear,\n",
    "\t\t'parameters':{\n",
    "\t\t\t'z_lens': truncnorm(-2.5,np.inf,loc=0.5,scale=0.2).rvs,\n",
    "\t\t\t'gamma': truncnorm(-(2./.2),np.inf,loc=2.0,scale=0.2).rvs,\n",
    "            'theta_E': truncnorm(-(.8/.15),np.inf,loc=0.8,scale=0.15).rvs,\n",
    "            'e1':norm(loc=0,scale=0.2).rvs,\n",
    "            'e2':norm(loc=0,scale=0.2).rvs,\n",
    "            # see cross_object below\n",
    "\t\t\t'center_x':None,\n",
    "\t\t\t'center_y':None,\n",
    "\t\t\t'gamma1':norm(loc=0,scale=0.12).rvs,\n",
    "            'gamma2':norm(loc=0,scale=0.12).rvs,\n",
    "\t\t\t'ra_0':0.0,\n",
    "\t\t\t'dec_0':0.0,\n",
    "\t\t}\n",
    "\t},\n",
    "    'source':{\n",
    "\t\t'class': SingleSersicSource,\n",
    "\t\t'parameters':{\n",
    "\t\t\t'z_source':truncnorm(-5,np.inf,loc=2.,scale=0.4).rvs,\n",
    "            # range: 20 to 27, centered at 23.5\n",
    "            'mag_app':truncnorm(-3./2.,3./2.,loc=23.5,scale=7./3.).rvs,\n",
    "\t\t\t'output_ab_zeropoint':output_ab_zeropoint,\n",
    "\t\t\t'R_sersic':truncnorm(-(.5/.5),np.inf,loc=0.5,scale=0.5).rvs,\n",
    "\t\t\t'n_sersic':truncnorm(-1.25,np.inf,loc=3.,scale=1.).rvs,\n",
    "\t\t\t'e1':truncnorm(-2.5,2.5,loc=0,scale=0.2).rvs,\n",
    "            'e2':truncnorm(-2.5,2.5,loc=0,scale=0.2).rvs,\n",
    "            # see cross_object below\n",
    "\t\t\t'center_x':None,\n",
    "\t\t\t'center_y':None}\n",
    "\n",
    "\t},\n",
    "    'lens_light':{\n",
    "\t\t'class': SingleSersicSource,\n",
    "\t\t'parameters':{\n",
    "\t\t\t'z_source':None,\n",
    "            # range: 17 to 23\n",
    "            'mag_app':truncnorm(-3./2.,3./2.,loc=20,scale=2.).rvs,\n",
    "\t\t\t'output_ab_zeropoint':output_ab_zeropoint,\n",
    "\t\t\t'R_sersic':truncnorm(-(1./.8),np.inf,loc=1.0,scale=0.8).rvs,\n",
    "\t\t\t'n_sersic':truncnorm(-1.25,np.inf,loc=3.,scale=2.).rvs,\n",
    "\t\t\t'e1':truncnorm(-2.5,2.5,loc=0,scale=0.2).rvs,\n",
    "            'e2':truncnorm(-2.5,2.5,loc=0,scale=0.2).rvs,\n",
    "            # see cross_object below\n",
    "\t\t\t'center_x':None,\n",
    "\t\t\t'center_y':None}\n",
    "\t},\n",
    "    'point_source':{\n",
    "\t\t'class': SinglePointSource,\n",
    "\t\t'parameters':{\n",
    "            # see cross_object below for z,x,y\n",
    "            'z_point_source':None,\n",
    "\t\t\t'x_point_source':None,\n",
    "\t\t\t'y_point_source':None,\n",
    "            # range: 19 to 25\n",
    "            'mag_app':truncnorm(-3./2.,3./2.,loc=22.,scale=2.).rvs,\n",
    "\t\t\t'output_ab_zeropoint':output_ab_zeropoint,\n",
    "\t\t\t'mag_pert': dist.MultipleValues(dist=truncnorm(-1/0.3,np.inf,1,0.3).rvs,num=10),\n",
    "            'compute_time_delays':False\n",
    "\t\t}\n",
    "\t},\n",
    "    'cosmology':{\n",
    "\t\t'parameters':{\n",
    "\t\t\t'cosmology_name': 'planck18'\n",
    "\t\t}\n",
    "\t},\n",
    "    'psf':{\n",
    "\t\t'parameters':{\n",
    "\t\t\t'psf_type':'PIXEL',\n",
    "\t\t\t'kernel_point_source':draw_psf_kernel,\n",
    "\t\t\t'point_source_supersampling_factor':1\n",
    "\t\t}\n",
    "\t},\n",
    "\t'detector':{\n",
    "\t\t'parameters':{\n",
    "\t\t\t'pixel_scale':0.04,'ccd_gain':1.5,'read_noise':3.0,\n",
    "\t\t\t'magnitude_zero_point':output_ab_zeropoint,\n",
    "\t\t\t'exposure_time':1400.,'sky_brightness':21.9,\n",
    "\t\t\t'num_exposures':1,'background_noise':None\n",
    "\t\t}\n",
    "\t},\n",
    "\t'drizzle':{\n",
    "\t\t'parameters':{\n",
    "        \t\t'supersample_pixel_scale':0.040,'output_pixel_scale':0.040,\n",
    "        \t\t'wcs_distortion':None,\n",
    "        \t\t'offset_pattern':[(0,0),(0.5,0.5)],\n",
    "        \t\t'psf_supersample_factor':1\n",
    "\t\t}\n",
    "\t},\n",
    "    'cross_object':{\n",
    "\t\t'parameters':{\n",
    "            ('main_deflector:center_x,lens_light:center_x'):dist.DuplicateScatter(\n",
    "                dist=norm(loc=0,scale=0.07).rvs,scatter=0.005),\n",
    "            ('main_deflector:center_y,lens_light:center_y'):dist.DuplicateScatter(\n",
    "                dist=norm(loc=0,scale=0.07).rvs,scatter=0.005),\n",
    "            ('source:center_x,source:center_y,point_source:x_point_source,'+\n",
    "                'point_source:y_point_source'):dist.DuplicateXY(\n",
    "                x_dist=norm(loc=0.0,scale=0.1).rvs,\n",
    "                y_dist=norm(loc=0.0,scale=0.1).rvs),\n",
    "\t\t\t('main_deflector:z_lens,lens_light:z_source,source:z_source,'+\n",
    "                 'point_source:z_point_source'):dist.RedshiftsPointSource(\n",
    "\t\t\t\tz_lens_min=0,z_lens_mean=0.5,z_lens_std=0.2,\n",
    "\t\t\t\tz_source_min=0,z_source_mean=2,z_source_std=0.4)\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for training the NPE network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "####################\n",
    "# Things that change\n",
    "####################\n",
    "\n",
    "training_folder = '/scratch/users/sydney3/paper_results/broad_training/'\n",
    "write_folder = '/scratch/users/sydney3/paper_results/broad_training/diag_no_R_src/'\n",
    "num_training_folders = 10\n",
    "\n",
    "input_norm_path = (write_folder + 'norms.csv')\n",
    "\n",
    "# loops thru training set\n",
    "n_epochs = 100\n",
    "# Steps Per Decay\n",
    "steps_per_decay = 1e3\n",
    "\n",
    "# A string with which loss function to use.\n",
    "loss_function = 'diag'\n",
    "\n",
    "# Whether or not to normalize the images by the standard deviation\n",
    "norm_images = False\n",
    "log_norm_images = True\n",
    "\n",
    "# Where to save the model weights\n",
    "model_weights = (write_folder +\n",
    "    'xresnet34_{epoch:03d}-{val_loss:.2f}.h5')\n",
    "\n",
    "model_weights_init = None\n",
    "#model_weights_init = (write_folder + 'xresnet34_165--19.18_last.h5')\n",
    "\n",
    "# The learning rate for the model\n",
    "learning_rate = 5e-4\n",
    "#learning_rate = 5e-3*(0.98**(165*5e5/(512*1e3)))\n",
    "# Whether or not to use random rotation of the input images\n",
    "\n",
    "random_rotation = True\n",
    "\n",
    "# Save training results to .csv file\n",
    "csv_path = (write_folder + 'log.csv')\n",
    "\n",
    "##########################\n",
    "# Things that don't change\n",
    "##########################\n",
    "batch_size = 512\n",
    "img_size = (80,80,1)\n",
    "learning_params = ['main_deflector_parameters_theta_E',\n",
    "        'main_deflector_parameters_gamma1','main_deflector_parameters_gamma2',\n",
    "        'main_deflector_parameters_gamma','main_deflector_parameters_e1',\n",
    "        'main_deflector_parameters_e2','main_deflector_parameters_center_x',\n",
    "        'main_deflector_parameters_center_y','source_parameters_center_x',\n",
    "        'source_parameters_center_y']\n",
    "flip_pairs = None\n",
    "weight_terms = None\n",
    "\n",
    "# prep training / validation paths\n",
    "folder_indices = range(0,num_training_folders)\n",
    "npy_folders_train = [\n",
    "        (training_folder+'train_%d/'%(i)) for i in folder_indices]\n",
    "tfr_train_paths = [\n",
    "        os.path.join(path,'data.tfrecord') for path in npy_folders_train]\n",
    "npy_folder_val = (training_folder+'validate/')\n",
    "tfr_val_path = os.path.join(npy_folder_val,'data.tfrecord')\n",
    "metadata_paths_train = [\n",
    "        os.path.join(path,'metadata.csv') for path in npy_folders_train]\n",
    "metadata_path_val = os.path.join(npy_folder_val,'metadata.csv')\n",
    "# The detector kwargs to use for on-the-fly noise generation\n",
    "kwargs_detector = None\n",
    "# A string specifying which model to use\n",
    "model_type = 'xresnet34'\n",
    "# A string specifying which optimizer to use\n",
    "optimizer = 'Adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating SNPE Training Sets ###\n",
    "\n",
    "See making_predictions.ipynb, which shows how these configuration files are\n",
    "generated from NPE predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example configuration for training an SNPE network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "####################\n",
    "# Things that change\n",
    "####################\n",
    "\n",
    "training_folder = '/scratch/users/sydney3/paper_results/sequential_training/shifted_test_set/narrow000/'\n",
    "write_folder = '/scratch/users/sydney3/paper_results/sequential_training/shifted_test_set/narrow000/'\n",
    "num_training_folders = 1\n",
    "\n",
    "# APT proposal\n",
    "proposal_means = np.asarray([ 0.567,  0.027,  0.167,  1.911,  0.173,  0.037, -0.018,  0.034, -0.035,  0.011,  0.728])\n",
    "proposal_prec = np.linalg.inv(np.diag(np.asarray([0.023, 0.039, 0.054, 0.11 , 0.093, 0.121, 0.009, 0.008, 0.007, 0.007, 0.284])**2))\n",
    "# APT prior\n",
    "prior_means = np.asarray([0.8,0.,0.,2.0,0.,0.,0.,0.,0.,0.,0.5])\n",
    "prior_prec = np.linalg.inv(np.diag(np.asarray([0.15,0.12,0.12,0.2,0.2,0.2,0.07,0.07,0.1,0.1,0.5])**2))\n",
    "\n",
    "input_norm_path = ('/scratch/users/sydney3/paper_results/broad_training/' + 'norms.csv')\n",
    "\n",
    "# loops thru training set\n",
    "n_epochs = 50\n",
    "# Steps Per Decay\n",
    "steps_per_decay = 100\n",
    "\n",
    "# A string with which loss function to use.\n",
    "loss_function = 'diagapt'\n",
    "\n",
    "# Whether or not to normalize the images by the standard deviation\n",
    "norm_images = False\n",
    "log_norm_images = True\n",
    "\n",
    "# Where to save the model weights\n",
    "model_weights = (write_folder +\n",
    "    'xresnet34_{epoch:03d}-{val_loss:.2f}.h5')\n",
    "\n",
    "model_weights_init = None\n",
    "#model_weights_init = ('/scratch/users/sydney3/paper_results/broad_training/'+\n",
    "#                      'xresnet34_053--14.10_best.h5')\n",
    "\n",
    "# The learning rate for the model picks up where it left off!!\n",
    "#learning_rate = 5e-4\n",
    "learning_rate = 5e-4*(0.98**(53*5e5/(512*1e3)))\n",
    "\n",
    "# Whether or not to use random rotation of the input images\n",
    "#  NEEDS TO BE TURNED OFF FOR SEQUENTIAL\n",
    "random_rotation = False\n",
    "\n",
    "# Save training results to .csv file\n",
    "csv_path = (write_folder + 'log.csv')\n",
    "\n",
    "##########################\n",
    "# Things that don't change\n",
    "##########################\n",
    "batch_size = 512\n",
    "img_size = (80,80,1)\n",
    "learning_params = ['main_deflector_parameters_theta_E',\n",
    "        'main_deflector_parameters_gamma1','main_deflector_parameters_gamma2',\n",
    "        'main_deflector_parameters_gamma','main_deflector_parameters_e1',\n",
    "        'main_deflector_parameters_e2','main_deflector_parameters_center_x',\n",
    "        'main_deflector_parameters_center_y','source_parameters_center_x',\n",
    "        'source_parameters_center_y','source_parameters_R_sersic']\n",
    "flip_pairs = None\n",
    "weight_terms = None\n",
    "\n",
    "# prep training / validation paths\n",
    "folder_indices = range(0,num_training_folders)\n",
    "npy_folders_train = [\n",
    "        (training_folder+'train_%d/'%(i)) for i in folder_indices]\n",
    "tfr_train_paths = [\n",
    "        os.path.join(path,'data.tfrecord') for path in npy_folders_train]\n",
    "npy_folder_val = (training_folder+'validate/')\n",
    "tfr_val_path = os.path.join(npy_folder_val,'data.tfrecord')\n",
    "metadata_paths_train = [\n",
    "        os.path.join(path,'metadata.csv') for path in npy_folders_train]\n",
    "metadata_path_val = os.path.join(npy_folder_val,'metadata.csv')\n",
    "# The detector kwargs to use for on-the-fly noise generation\n",
    "kwargs_detector = None\n",
    "# A string specifying which model to use\n",
    "model_type = 'xresnet34'\n",
    "# A string specifying which optimizer to use\n",
    "optimizer = 'Adam'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
